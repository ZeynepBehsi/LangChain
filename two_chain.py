"""
Temperature farkÄ±nÄ± gÃ¶stermek iÃ§in iki farklÄ± Claude modeli ile iki chain oluÅŸturma.
"""

# import libraries
from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 2. Environment variables yÃ¼kle
load_dotenv()

# 3. Ä°ki farklÄ± model oluÅŸtur - temperature farkÄ± ile

# YaratÄ±cÄ± Claude
llm_creative = ChatAnthropic(
    model="claude-3-haiku-20240307",
    temperature=1.0  # Maksimum yaratÄ±cÄ±lÄ±k
)

# Deterministik Claude
llm_strict = ChatAnthropic(
    model="claude-3-haiku-20240307",
    temperature=0  # SÄ±fÄ±r randomness
)


# 4. Prompt template oluÅŸtur
print("ğŸ“ Prompt template hazÄ±rlanÄ±yor...")
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant specialized in explaining technical concepts clearly and concisely."),
    ("user", "{input}")
])

# 5. Output parser
# ne yapar: AI'Ä±n cevabÄ±nÄ± string formatÄ±na Ã§evirir, cevaptaki sadece metin kÄ±smÄ±nÄ± alÄ±r. 
output_parser = StrOutputParser()


# 6. Ä°ki chain oluÅŸtur
chain_creative = prompt | llm_creative | output_parser
chain_strict = prompt | llm_strict | output_parser

# KarÅŸÄ±laÅŸtÄ±r
question = "Tell me a story about AI and graphs."

print("ğŸ¨ CREATIVE (temp=1.0):")
print(chain_creative.invoke({"input": question}))

print("\nğŸ“ STRICT (temp=0):")
print(chain_strict.invoke({"input": question}))

