# ğŸ¯ LangChain INDEXES - DetaylÄ± Rehber

## Ä°Ã§indekiler
1. [Index Nedir?](#index-nedir)
2. [Index BileÅŸenleri](#index-bileÅŸenleri)
3. [Index OluÅŸturma](#index-oluÅŸturma)
4. [Arama YÃ¶ntemleri](#arama-yÃ¶ntemleri)
5. [Retriever KullanÄ±mÄ±](#retriever-kullanÄ±mÄ±)
6. [FarklÄ± Vector Store'lar](#farklÄ±-vector-storelar)
7. [Ä°leri Seviye](#ileri-seviye)
8. [Best Practices](#best-practices)

---

## Index Nedir?

**Index**, dÃ¶kÃ¼manlarÄ±n organize edilmiÅŸ ve hÄ±zlÄ± arama yapÄ±labilir halidir.

### GÃ¼nlÃ¼k Hayattan Analoji

```
ğŸ“– Kitap Ä°ndeksi:
   "Python" kelimesini ara
   â†’ Ä°ndekse bak
   â†’ Sayfa 42, 78, 156
   â†’ Sayfaya git

ğŸ” LangChain Ä°ndeksi:
   "Python nedir?" sorusunu sor
   â†’ Index'te ara
   â†’ Ä°lgili 3 dÃ¶kÃ¼manÄ± bul
   â†’ DÃ¶kÃ¼manlarÄ± dÃ¶ndÃ¼r
```

### Neden Index?

```python
# âŒ Index OLMADAN (her seferinde baÅŸtan tara)
for doc in 1_000_000_documents:
    if doc.contains(query):
        results.append(doc)
# â±ï¸ Ã‡ok yavaÅŸ! O(n)

# âœ… Index Ä°LE (Ã¶nceden hazÄ±rlanmÄ±ÅŸ)
results = index.search(query)
# âš¡ Ã‡ok hÄ±zlÄ±! O(log n) veya O(1)
```

---

## Index BileÅŸenleri

LangChain Index 3 ana bileÅŸenden oluÅŸur:

### 1. Documents (DÃ¶kÃ¼manlar)

```python
from langchain.schema import Document

doc = Document(
    page_content="LangChain bir LLM framework'Ã¼dÃ¼r.",
    metadata={
        "source": "langchain.com",
        "author": "Harrison Chase",
        "date": "2024-01-15",
        "category": "tutorial"
    }
)

# Document yapÄ±sÄ±:
# â”œâ”€ page_content: str (dÃ¶kÃ¼man iÃ§eriÄŸi)
# â””â”€ metadata: dict (ek bilgiler)
```

### 2. Embeddings (VektÃ¶r Temsilleri)

```python
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2"
)

# Text â†’ Vector dÃ¶nÃ¼ÅŸÃ¼mÃ¼
text = "LangChain bir framework'tÃ¼r"
vector = embeddings.embed_query(text)

# vector = [0.123, -0.456, 0.789, ..., 0.234]
# 768 boyutlu sayÄ±sal vektÃ¶r
```

**Embedding BoyutlarÄ±:**
```
Model                        Boyut    KullanÄ±m
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all-mpnet-base-v2            768     Genel amaÃ§lÄ±
text-embedding-ada-002       1536    OpenAI (Ã¼cretli)
all-MiniLM-L6-v2             384     HÄ±zlÄ±, hafif
instructor-xl                768     Ã–zelleÅŸtirilebilir
```

### 3. Vector Store (Depolama)

```python
from langchain_community.vectorstores import Chroma

vectorstore = Chroma(
    persist_directory="./my_index",
    embedding_function=embeddings
)

# Vector Store iÅŸlevleri:
# â”œâ”€ VektÃ¶rleri saklar
# â”œâ”€ Similarity search yapar
# â”œâ”€ Metadata filtreler
# â””â”€ Persistent (kalÄ±cÄ±) depolama
```

---

## Index OluÅŸturma

### YÃ¶ntem 1: from_documents() - En YaygÄ±n

```python
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document

# 1. DÃ¶kÃ¼manlar oluÅŸtur
documents = [
    Document(page_content="Python bir programlama dilidir."),
    Document(page_content="JavaScript web iÃ§in kullanÄ±lÄ±r."),
    Document(page_content="Machine Learning AI'Ä±n bir dalÄ±dÄ±r.")
]

# 2. Embedding modeli
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-mpnet-base-v2"
)

# 3. Index oluÅŸtur (tek satÄ±r!)
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    persist_directory="./my_index"
)

# âœ… Index hazÄ±r!
```

### YÃ¶ntem 2: from_texts() - Text Listesinden

```python
texts = [
    "Python bir programlama dilidir.",
    "JavaScript web iÃ§in kullanÄ±lÄ±r.",
    "Machine Learning AI'Ä±n bir dalÄ±dÄ±r."
]

metadatas = [
    {"source": "python.txt"},
    {"source": "javascript.txt"},
    {"source": "ml.txt"}
]

vectorstore = Chroma.from_texts(
    texts=texts,
    metadatas=metadatas,
    embedding=embeddings
)
```

### YÃ¶ntem 3: add_documents() - Mevcut Index'e Ekle

```python
# Mevcut index'i yÃ¼kle
vectorstore = Chroma(
    persist_directory="./my_index",
    embedding_function=embeddings
)

# Yeni dÃ¶kÃ¼manlar ekle
new_docs = [
    Document(page_content="React bir UI kÃ¼tÃ¼phanesidir.")
]

vectorstore.add_documents(new_docs)
```

### YÃ¶ntem 4: BÃ¼yÃ¼k Veri - Batch Ä°ÅŸleme

```python
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. KlasÃ¶rdeki tÃ¼m dosyalarÄ± yÃ¼kle
loader = DirectoryLoader('./data/', glob="**/*.txt")
raw_documents = loader.load()

# 2. DÃ¶kÃ¼manlarÄ± parÃ§ala
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
documents = text_splitter.split_documents(raw_documents)

# 3. Batch'ler halinde index'e ekle
batch_size = 100
for i in range(0, len(documents), batch_size):
    batch = documents[i:i+batch_size]
    vectorstore.add_documents(batch)
    print(f"âœ… {i+len(batch)}/{len(documents)} dÃ¶kÃ¼man eklendi")
```

---

## Arama YÃ¶ntemleri

### 1. similarity_search() - Temel Arama

```python
# En basit kullanÄ±m
results = vectorstore.similarity_search(
    query="Python nedir?",
    k=3  # En iyi 3 sonuÃ§
)

for doc in results:
    print(doc.page_content)

# Ã‡Ä±ktÄ±:
# Python bir programlama dilidir.
# Machine Learning AI'Ä±n bir dalÄ±dÄ±r.
# JavaScript web iÃ§in kullanÄ±lÄ±r.
```

**Parametreler:**
- `query`: str - Arama sorgusu
- `k`: int - KaÃ§ sonuÃ§ dÃ¶ndÃ¼rÃ¼lsÃ¼n (varsayÄ±lan: 4)
- `filter`: dict - Metadata filtresi (opsiyonel)

### 2. similarity_search_with_score() - Skorlu Arama

```python
results = vectorstore.similarity_search_with_score(
    query="Machine Learning nedir?",
    k=2
)

for doc, score in results:
    print(f"Score: {score:.4f}")
    print(f"Content: {doc.page_content}")
    print()

# Ã‡Ä±ktÄ±:
# Score: 0.2341  â† DÃ¼ÅŸÃ¼k = daha benzer
# Content: Machine Learning AI'Ä±n bir dalÄ±dÄ±r.
#
# Score: 0.8765  â† YÃ¼ksek = daha farklÄ±
# Content: Python bir programlama dilidir.
```

**Not:** Chroma'da score = distance (uzaklÄ±k)
- 0'a yakÄ±n = Ã§ok benzer âœ…
- 1'e yakÄ±n = Ã§ok farklÄ± âŒ

### 3. max_marginal_relevance_search() - MMR

```python
# MMR = Maximum Marginal Relevance
# Hem alakalÄ± hem de birbirinden farklÄ± sonuÃ§lar

results = vectorstore.max_marginal_relevance_search(
    query="programlama dilleri",
    k=3,
    fetch_k=10,      # 10 aday getir
    lambda_mult=0.5  # 0.5 = denge (alakalÄ±lÄ±k + Ã§eÅŸitlilik)
)

# lambda_mult:
# 0.0 = Maksimum Ã§eÅŸitlilik (birbirinden farklÄ±)
# 1.0 = Maksimum alakalÄ±lÄ±k (similarity'ye eÅŸit)
# 0.5 = Denge
```

**Ne zaman kullanÄ±lÄ±r?**
- Ã‡eÅŸitli bakÄ±ÅŸ aÃ§Ä±larÄ± istediÄŸinizde
- Tekrar eden bilgilerden kaÃ§Ä±nmak iÃ§in
- Ã–zet Ã§Ä±karma iÃ§in

### 4. similarity_search_by_vector() - VektÃ¶r ile Arama

```python
# Ã–nce bir text'i vektÃ¶re Ã§evir
query_vector = embeddings.embed_query("Python nedir?")

# VektÃ¶r ile ara
results = vectorstore.similarity_search_by_vector(
    embedding=query_vector,
    k=2
)
```

---

## Retriever KullanÄ±mÄ±

### Retriever Nedir?

```
Vector Store vs Retriever:

Vector Store:
â”œâ”€ Ham depolama
â”œâ”€ BirÃ§ok farklÄ± arama metodu
â””â”€ Esnek ama tutarlÄ± deÄŸil

Retriever:
â”œâ”€ Standart arayÃ¼z
â”œâ”€ LangChain chain'lerle uyumlu
â””â”€ TutarlÄ± API
```

### Temel Retriever

```python
# Vector store'dan retriever oluÅŸtur
retriever = vectorstore.as_retriever()

# KullanÄ±mÄ± Ã§ok basit
docs = retriever.invoke("Python nedir?")

# veya
docs = retriever.get_relevant_documents("Python nedir?")
```

### Retriever KonfigÃ¼rasyonu

```python
retriever = vectorstore.as_retriever(
    search_type="similarity",  # Arama tipi
    search_kwargs={            # Arama parametreleri
        "k": 4,               # En iyi 4 sonuÃ§
        "score_threshold": 0.5 # Minimum skor
    }
)
```

### Retriever Tipleri

#### 1. Similarity Retriever (VarsayÄ±lan)

```python
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

# Basit benzerlik aramasÄ±
```

#### 2. MMR Retriever

```python
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 4,              # 4 sonuÃ§ dÃ¶ndÃ¼r
        "fetch_k": 20,       # 20 aday getir
        "lambda_mult": 0.5   # Denge
    }
)

# Ã‡eÅŸitli sonuÃ§lar iÃ§in
```

#### 3. Score Threshold Retriever

```python
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.7,  # 0.7'den dÃ¼ÅŸÃ¼k skorlar (daha benzer)
        "k": 5
    }
)

# Sadece yeterince benzer olanlarÄ± dÃ¶ndÃ¼r
# EÅŸiÄŸi geÃ§emezse boÅŸ liste dÃ¶ner
```

### Retriever ile Chain KullanÄ±mÄ±

```python
from langchain.chains import RetrievalQA
from langchain_anthropic import ChatAnthropic

# LLM
llm = ChatAnthropic(model="claude-3-haiku-20240307")

# Retriever
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# Chain oluÅŸtur
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever
)

# Kullan
answer = qa_chain.invoke("Python ne zaman geliÅŸtirildi?")
print(answer)
```

---

## FarklÄ± Vector Store'lar

### 1. Chroma (Yerel, Ãœcretsiz)

```python
from langchain_community.vectorstores import Chroma

vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)
```

**Ã–zellikler:**
- âœ… Tamamen Ã¼cretsiz
- âœ… Yerel Ã§alÄ±ÅŸÄ±r (internet gerektirmez)
- âœ… Persistent (kalÄ±cÄ±) depolama
- âœ… Metadata filtreleme
- âŒ BÃ¼yÃ¼k Ã¶lÃ§ek iÃ§in yavaÅŸ
- âŒ DaÄŸÄ±tÄ±k mimari yok

### 2. FAISS (Facebook AI, HÄ±zlÄ±)

```python
from langchain_community.vectorstores import FAISS

vectorstore = FAISS.from_documents(
    documents=documents,
    embedding=embeddings
)

# Kaydet
vectorstore.save_local("./faiss_index")

# YÃ¼kle
vectorstore = FAISS.load_local(
    "./faiss_index",
    embeddings=embeddings
)
```

**Ã–zellikler:**
- âœ… Ã‡ok hÄ±zlÄ±
- âœ… Milyonlarca vektÃ¶r destekler
- âœ… GPU desteÄŸi
- âŒ Metadata filtreleme sÄ±nÄ±rlÄ±
- âŒ Real-time gÃ¼ncelleme zor

### 3. Pinecone (Cloud, Ãœcretli)

```python
from langchain_community.vectorstores import Pinecone
import pinecone

# Initialize
pinecone.init(
    api_key="your-api-key",
    environment="us-west1-gcp"
)

# Index oluÅŸtur
vectorstore = Pinecone.from_documents(
    documents=documents,
    embedding=embeddings,
    index_name="my-index"
)
```

**Ã–zellikler:**
- âœ… DaÄŸÄ±tÄ±k mimari
- âœ… Otomatik Ã¶lÃ§ekleme
- âœ… Real-time gÃ¼ncelleme
- âœ… Metadata filtreleme
- âŒ Ãœcretli
- âŒ Ä°nternet gerektirir

### 4. Weaviate (Hybrid Search)

```python
from langchain_community.vectorstores import Weaviate
import weaviate

client = weaviate.Client("http://localhost:8080")

vectorstore = Weaviate(
    client=client,
    index_name="MyIndex",
    text_key="text",
    embedding=embeddings
)
```

**Ã–zellikler:**
- âœ… Hybrid search (vector + keyword)
- âœ… GraphQL API
- âœ… Schema tanÄ±mlama
- âŒ Kurulum karmaÅŸÄ±k

### Vector Store KarÅŸÄ±laÅŸtÄ±rma

| Store      | Ãœcretsiz | HÄ±z    | Ã–lÃ§ek    | Metadata | Use Case        |
|------------|----------|--------|----------|----------|-----------------|
| Chroma     | âœ…       | Orta   | KÃ¼Ã§Ã¼k    | âœ…       | Prototip, Demo  |
| FAISS      | âœ…       | âš¡HÄ±zlÄ± | BÃ¼yÃ¼k    | âš ï¸       | Yerel, HÄ±zlÄ±    |
| Pinecone   | âŒ       | âš¡HÄ±zlÄ± | Ã‡ok BÃ¼yÃ¼k| âœ…       | Production      |
| Weaviate   | âœ…       | HÄ±zlÄ±  | BÃ¼yÃ¼k    | âœ…       | Hybrid Search   |
| Qdrant     | âœ…       | HÄ±zlÄ±  | BÃ¼yÃ¼k    | âœ…       | Production      |

---

## Ä°leri Seviye

### 1. Metadata Filtreleme

```python
# Index oluÅŸtururken metadata ekle
documents = [
    Document(
        page_content="Python gÃ¼Ã§lÃ¼ bir dildir",
        metadata={"language": "python", "level": "beginner", "year": 2024}
    ),
    Document(
        page_content="JavaScript async Ã§alÄ±ÅŸÄ±r",
        metadata={"language": "javascript", "level": "intermediate", "year": 2024}
    ),
    Document(
        page_content="Rust memory-safe'tir",
        metadata={"language": "rust", "level": "advanced", "year": 2023}
    )
]

vectorstore = Chroma.from_documents(documents, embeddings)

# Filtreyle ara
results = vectorstore.similarity_search(
    query="gÃ¼Ã§lÃ¼ dil",
    k=2,
    filter={"language": "python"}  # Sadece Python dÃ¶kÃ¼manlarÄ±
)

# Birden fazla filtre
results = vectorstore.similarity_search(
    query="modern diller",
    k=3,
    filter={
        "year": 2024,
        "level": {"$in": ["beginner", "intermediate"]}
    }
)
```

**Filter OperatÃ¶rleri:**
```python
# EÅŸitlik
filter={"language": "python"}

# IN operatÃ¶rÃ¼ (Chroma)
filter={"language": {"$in": ["python", "javascript"]}}

# SayÄ±sal karÅŸÄ±laÅŸtÄ±rma
filter={"year": {"$gte": 2023}}  # >= 2023

# AND (birden fazla field)
filter={"language": "python", "level": "beginner"}

# OR (Chroma - $or)
filter={"$or": [
    {"language": "python"},
    {"language": "rust"}
]}
```

### 2. Ã–zel Embedding Fonksiyonu

```python
from langchain.embeddings.base import Embeddings
from typing import List

class CustomEmbedding(Embeddings):
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        # Toplu dÃ¶kÃ¼man embedding'i
        return [self.embed_query(text) for text in texts]
    
    def embed_query(self, text: str) -> List[float]:
        # Tek query embedding'i
        # Kendi modelinizi buraya
        return custom_model.encode(text)

# Kullan
custom_embeddings = CustomEmbedding()
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=custom_embeddings
)
```

### 3. Index Merge (BirleÅŸtirme)

```python
# Ä°ki farklÄ± index'i birleÅŸtir
vectorstore1 = Chroma(persist_directory="./index1", ...)
vectorstore2 = Chroma(persist_directory="./index2", ...)

# Index2'deki dÃ¶kÃ¼manlarÄ± index1'e ekle
docs = vectorstore2.similarity_search("", k=1000)  # TÃ¼m dÃ¶kÃ¼manlarÄ± al
vectorstore1.add_documents(docs)
```

### 4. Index GÃ¼ncelleme (Update)

```python
# Chroma'da direkt update yok, silip tekrar eklemek gerek

# 1. Ã–nce sil
vectorstore.delete(ids=["doc_id_1", "doc_id_2"])

# 2. GÃ¼ncellenmiÅŸ versiyonu ekle
updated_docs = [
    Document(
        page_content="GÃ¼ncellenmiÅŸ iÃ§erik",
        metadata={"id": "doc_id_1", "version": 2}
    )
]
vectorstore.add_documents(updated_docs)
```

### 5. Batch Processing (BÃ¼yÃ¼k Veri)

```python
import time

def index_large_dataset(documents, batch_size=100):
    """
    BÃ¼yÃ¼k veri setlerini batch'ler halinde index'le
    """
    vectorstore = Chroma(
        persist_directory="./large_index",
        embedding_function=embeddings
    )
    
    total = len(documents)
    for i in range(0, total, batch_size):
        batch = documents[i:i+batch_size]
        
        # Batch ekle
        vectorstore.add_documents(batch)
        
        # Ä°lerleme gÃ¶ster
        progress = min(i + batch_size, total)
        print(f"âœ… {progress}/{total} ({100*progress/total:.1f}%)")
        
        # Rate limiting (gerekirse)
        time.sleep(0.1)
    
    return vectorstore

# Kullan
docs = load_million_documents()  # 1M dÃ¶kÃ¼man
vectorstore = index_large_dataset(docs, batch_size=500)
```

---

## Best Practices

### 1. Chunk Size Optimizasyonu

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# âŒ KÃ–TÃœ: Ã‡ok bÃ¼yÃ¼k chunk'lar
splitter = RecursiveCharacterTextSplitter(
    chunk_size=5000,  # Ã‡ok bÃ¼yÃ¼k!
    chunk_overlap=0
)
# Sorun: AlakasÄ±z bilgiler dahil olur

# âœ… Ä°YÄ°: Optimum boyut
splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,   # LLM context'ine uygun
    chunk_overlap=50  # Bilgi kaybÄ±nÄ± Ã¶nler
)
```

**Chunk Size Rehberi:**
```
DÃ¶kÃ¼man Tipi          Chunk Size    Overlap
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KÄ±sa cevaplar (FAQ)   200-300      20-30
Genel metinler        500-1000     50-100
Teknik dÃ¶kÃ¼manlar     1000-1500    100-150
Kodlar                300-500      50
```

### 2. Metadata Stratejisi

```python
# âœ… Ä°YÄ°: Zengin metadata
Document(
    page_content="...",
    metadata={
        "source": "docs/api.md",
        "title": "API Reference",
        "section": "Authentication",
        "category": "backend",
        "tags": ["api", "auth", "security"],
        "created_at": "2024-01-15",
        "version": "2.0",
        "author": "John Doe"
    }
)

# ArtÄ±k Ã§ok detaylÄ± filtreleme yapabilirsiniz:
results = vectorstore.similarity_search(
    query="authentication",
    filter={
        "category": "backend",
        "version": "2.0",
        "tags": {"$in": ["auth", "security"]}
    }
)
```

### 3. Index BakÄ±mÄ±

```python
# Periyodik olarak index'i optimize edin

def maintain_index(vectorstore):
    """Index bakÄ±m rutini"""
    
    # 1. Eski dÃ¶kÃ¼manlarÄ± temizle
    old_doc_ids = get_old_document_ids()
    vectorstore.delete(ids=old_doc_ids)
    
    # 2. Duplicate'leri kaldÄ±r
    remove_duplicates(vectorstore)
    
    # 3. Index istatistikleri
    stats = get_index_stats(vectorstore)
    print(f"Index size: {stats['count']} documents")
    
    # 4. Persist
    vectorstore.persist()

# Haftada bir Ã§alÄ±ÅŸtÄ±r
maintain_index(vectorstore)
```

### 4. Error Handling

```python
from langchain_community.vectorstores import Chroma

def safe_index_creation(documents, embeddings):
    """Hata yÃ¶netimli index oluÅŸturma"""
    try:
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory="./safe_index"
        )
        print("âœ… Index oluÅŸturuldu")
        return vectorstore
        
    except Exception as e:
        print(f"âŒ Hata: {e}")
        
        # Fallback: Batch'ler halinde dene
        print("ğŸ”„ Batch iÅŸleme deneniyor...")
        vectorstore = Chroma(
            persist_directory="./safe_index",
            embedding_function=embeddings
        )
        
        for i in range(0, len(documents), 100):
            try:
                batch = documents[i:i+100]
                vectorstore.add_documents(batch)
                print(f"âœ… Batch {i//100 + 1} eklendi")
            except Exception as batch_error:
                print(f"âŒ Batch {i//100 + 1} hatasÄ±: {batch_error}")
                continue
        
        return vectorstore
```

### 5. Monitoring & Logging

```python
import logging
from datetime import datetime

# Logger ayarla
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MonitoredVectorStore:
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
        self.search_count = 0
        self.avg_results = []
    
    def search(self, query, k=4):
        """MonitÃ¶rlÃ¼ arama"""
        start_time = datetime.now()
        
        results = self.vectorstore.similarity_search(query, k=k)
        
        # Metrikleri kaydet
        elapsed = (datetime.now() - start_time).total_seconds()
        self.search_count += 1
        self.avg_results.append(len(results))
        
        logger.info(f"Search #{self.search_count}: '{query}' - "
                   f"{len(results)} results in {elapsed:.3f}s")
        
        return results
    
    def get_stats(self):
        """Ä°statistikleri gÃ¶ster"""
        return {
            "total_searches": self.search_count,
            "avg_results": sum(self.avg_results) / len(self.avg_results)
        }

# Kullan
monitored = MonitoredVectorStore(vectorstore)
monitored.search("Python")
print(monitored.get_stats())
```

---

## Ã–zet ve KarÅŸÄ±laÅŸtÄ±rma

### Index vs Raw Text

```
Zaman KarmaÅŸÄ±klÄ±ÄŸÄ±:

Raw Text Arama:
â”œâ”€ O(n) - Her dÃ¶kÃ¼man taranÄ±r
â”œâ”€ 1M dÃ¶kÃ¼man = 1M iÅŸlem
â””â”€ Ã‡ok yavaÅŸ âŒ

Index Arama:
â”œâ”€ O(log n) - Binary search benzeri
â”œâ”€ 1M dÃ¶kÃ¼man = ~20 iÅŸlem
â””â”€ Ã‡ok hÄ±zlÄ± âœ…
```

### Index KullanÄ±m SenaryolarÄ±

```python
# âœ… Index KULLAN
scenarios = [
    "RAG (Retrieval Augmented Generation)",
    "Question Answering sistemi",
    "Semantic Search",
    "Document Retrieval",
    "Knowledge Base arama",
    "Chatbot context yÃ¶netimi"
]

# âŒ Index GEREKMÄ°YOR
scenarios = [
    "Tek bir kÄ±sa dÃ¶kÃ¼man",
    "Static cevaplar (hardcoded)",
    "Basit keyword matching",
    "Tam metin aramaya gerek yok"
]
```

### Hangi Vector Store?

```python
# ğŸ  Prototip/Development
use_case = "HÄ±zlÄ± prototip"
solution = "Chroma (Ã¼cretsiz, kolay)"

# ğŸš€ Production (KÃ¼Ã§Ã¼k/Orta Ã–lÃ§ek)
use_case = "Production app, <1M dÃ¶kÃ¼man"
solution = "Chroma veya FAISS (self-hosted)"

# ğŸŒ Production (BÃ¼yÃ¼k Ã–lÃ§ek)
use_case = "Enterprise, >1M dÃ¶kÃ¼man, daÄŸÄ±tÄ±k"
solution = "Pinecone, Weaviate, veya Qdrant"

# âš¡ Maksimum HÄ±z
use_case = "Latency kritik, GPU var"
solution = "FAISS (GPU mode)"

# ğŸ” Hybrid Search
use_case = "Hem vector hem keyword search"
solution = "Weaviate veya Qdrant"
```

---

## Sonraki AdÄ±mlar

1. âœ… `indexes_ornekleri.py` Ã§alÄ±ÅŸtÄ±rÄ±n
2. ğŸ“– Kendi dÃ¶kÃ¼manlarÄ±nÄ±zla index oluÅŸturun
3. ğŸ” FarklÄ± retriever tiplerini deneyin
4. ğŸ“Š Metadata filtreleme kullanÄ±n
5. ğŸš€ Production'a hazÄ±rlanÄ±n

**BaÅŸarÄ±lar! ğŸ‰**
